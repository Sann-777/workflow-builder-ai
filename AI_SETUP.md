# AI Integration Setup Guide

## Overview

The Workflow Builder uses OpenAI's GPT-4o-mini model to generate workflows from natural language descriptions. This guide explains how the AI integration works and how to configure it.

## How It Works

### Backend Implementation

1. **OpenAI Client**: Direct integration using `openai` Python SDK (not PydanticAI)
2. **httpx Client**: Explicit httpx.Client() to avoid Lambda proxy errors
3. **Fallback**: If AI fails or API key is missing, falls back to mock workflow generation

### Environment Variables

The Lambda function requires these environment variables:

- `OPENAI_API_KEY`: Your OpenAI API key (required for AI features)
- `AI_MODEL`: Model to use (default: `gpt-4o-mini`)
- `APP_NAME`: Application name
- `APP_VERSION`: Version number
- `DEBUG`: Enable debug mode (true/false)
- `ALLOWED_ORIGINS`: CORS allowed origins
- `ENVIRONMENT`: Deployment environment (production/testing)

## GitHub Secrets Configuration

To enable AI features in GitHub Actions deployments:

1. Go to your GitHub repository
2. Navigate to **Settings** → **Secrets and variables** → **Actions**
3. Click **New repository secret**
4. Add the following secret:
   - **Name**: `OPENAI_API_KEY`
   - **Value**: Your OpenAI API key (starts with `sk-...`)

### Required Secrets

| Secret Name | Description | Required |
|------------|-------------|----------|
| `AWS_ACCESS_KEY_ID` | AWS access key | Yes |
| `AWS_SECRET_ACCESS_KEY` | AWS secret key | Yes |
| `OPENAI_API_KEY` | OpenAI API key | Optional (for AI features) |

## How GitHub Actions Uses the API Key

The deployment workflow (`.github/workflows/deploy.yml`) automatically:

1. **Passes to Terraform** (line 77):
   ```yaml
   terraform apply -auto-approve \
     -var="openai_api_key=${{ secrets.OPENAI_API_KEY || '' }}"
   ```

2. **Sets in Lambda Environment** (line 128):
   ```yaml
   OPENAI_API_KEY='${{ secrets.OPENAI_API_KEY || '' }}'
   ```

## Local Development

For local testing with AI enabled:

```bash
# Set your OpenAI API key
export OPENAI_API_KEY='sk-your-api-key-here'

# Run the backend
cd backend
uvicorn app.main:app --reload
```

## Manual Deployment

When deploying manually with `./scripts/deploy-all.sh`:

```bash
# Set your OpenAI API key
export OPENAI_API_KEY='sk-your-api-key-here'

# Deploy
./scripts/deploy-all.sh
```

The script will automatically pass the API key to Lambda.

## Docker Build for Lambda

Both GitHub Actions and the deployment script use Docker to build Lambda packages:

```bash
docker run --platform linux/amd64 --rm \
  -v "$PWD":/var/task \
  -w /var/task \
  public.ecr.aws/lambda/python:3.11 \
  pip install -r requirements-lambda.txt -t package
```

This ensures:
- ✅ Linux x86_64 compatible binaries (pydantic-core, etc.)
- ✅ No "No module named '_pydantic_core'" errors
- ✅ No architecture mismatch issues

## Verifying AI is Enabled

After deployment, check if AI is working:

```bash
# Get your API URL from Terraform outputs or AWS Console
API_URL="https://your-api-id.execute-api.us-east-1.amazonaws.com/prod"

# Check health endpoint
curl $API_URL/

# Expected response:
# {
#   "message": "Workflow Builder API is running",
#   "ai_enabled": true,        ← Should be true!
#   "ai_model": "gpt-4o-mini"
# }
```

## Testing AI Workflow Generation

```bash
curl -X POST $API_URL/generate_workflow \
  -H "Content-Type: application/json" \
  -d '{"description":"order processing workflow with payment verification"}'
```

Expected: A complete workflow JSON with nodes and edges generated by AI.

## Troubleshooting

### AI is disabled (`ai_enabled: false`)

**Possible causes:**
1. ❌ OPENAI_API_KEY not set in GitHub Secrets
2. ❌ OPENAI_API_KEY not set in Lambda environment
3. ❌ OpenAI SDK import failed

**Solution:**
1. Add OPENAI_API_KEY to GitHub Secrets
2. Redeploy using GitHub Actions or `./scripts/deploy-all.sh`
3. Check Lambda logs: `aws logs tail /aws/lambda/your-function-name --follow`

### Import errors in Lambda

**Error**: `No module named '_pydantic_core'`

**Cause**: Lambda package built on macOS ARM64 instead of Linux x86_64

**Solution**: Ensure Docker is running and deployment script uses Docker build

### Proxy errors

**Error**: `Client.__init__() got an unexpected keyword argument 'proxies'`

**Cause**: httpx client not initialized properly

**Solution**: Already fixed in `backend/app/main.py` with explicit httpx.Client()

## Cost Optimization

Using `gpt-4o-mini` instead of `gpt-4`:
- **gpt-4o-mini**: ~$0.15 per 1M input tokens, ~$0.60 per 1M output tokens
- **gpt-4**: ~$30 per 1M input tokens, ~$60 per 1M output tokens

Average workflow generation cost: **~$0.01-0.02** with gpt-4o-mini

## Architecture

```
User Request
    ↓
API Gateway
    ↓
Lambda Function
    ↓
OpenAI Client (with httpx)
    ↓
OpenAI API (gpt-4o-mini)
    ↓
Structured JSON Response
    ↓
WorkflowModel (Pydantic validation)
    ↓
Return to User
```

## Key Files

- `backend/app/main.py`: OpenAI integration logic
- `backend/requirements-lambda.txt`: Lambda dependencies (includes `openai`)
- `.github/workflows/deploy.yml`: GitHub Actions deployment
- `scripts/deploy-all.sh`: Manual deployment script
- `infrastructure/main.tf`: Lambda environment configuration

## Summary

✅ **OpenAI API key** stored in GitHub Secrets  
✅ **Automatic deployment** via GitHub Actions  
✅ **Docker builds** ensure Linux compatibility  
✅ **Fallback** to mock workflows if AI unavailable  
✅ **Cost-effective** using gpt-4o-mini  
